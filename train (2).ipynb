{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e268f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for dataset loading\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdd80ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dir = \"../dataset/crop_224/\"\n",
    "# labels_dir = \"../dataset/AppleV_intrinsic.csv\"\n",
    "\n",
    "# class AppleDataset(Dataset):\n",
    "#     def __init__(self, images_dir, labels_dir):\n",
    "#         self.labels = pd.read_csv(labels_dir)\n",
    "#         self.images = os.listdir(images_dir)\n",
    "\n",
    "#         # load the images and labels in a list\n",
    "#         self.img_data = []\n",
    "#         self.img_labels = []\n",
    "\n",
    "#         for image in self.images:\n",
    "#             img_path = os.path.join(images_dir, image)\n",
    "#             img = cv2.imread(img_path)\n",
    "#             img = np.transpose(img, [2, 0, 1])\n",
    "#             img = np.float32(img) / 255.0\n",
    "#             if img.shape != (3, 224, 224):\n",
    "#                 print(image)\n",
    "#                 continue\n",
    "#             self.img_data.append(img)\n",
    "\n",
    "#             image = image.split(\"_\")[0]+'.jpg'\n",
    "#             t = self.labels[self.labels['filenames'] == image][['TSS_brix', 'Firmness', 'Titerabile acid percentage']].values\n",
    "#             t = t.astype(np.float32).flatten()\n",
    "\n",
    "#             t = n\n",
    "#             self.img_labels.append(t)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def arguement(self, img, rotTimes, vFlip, hFlip):\n",
    "#         # Random rotation\n",
    "#         for j in range(rotTimes):\n",
    "#             img = np.rot90(img.copy(), axes=(1, 2))\n",
    "#         # Random vertical Flip\n",
    "#         for j in range(vFlip):\n",
    "#             img = img[:, :, ::-1].copy()\n",
    "#         # Random horizontal Flip\n",
    "#         for j in range(hFlip):\n",
    "#             img = img[:, ::-1, :].copy()\n",
    "#         return img\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = self.img_data[idx]\n",
    "#         label = self.img_labels[idx]\n",
    "\n",
    "#         rotTimes = random.randint(0, 3)\n",
    "#         vFlip = random.randint(0, 1)\n",
    "#         hFlip = random.randint(0, 1)\n",
    "#         image = self.arguement(image, rotTimes, vFlip, hFlip)\n",
    "        \n",
    "#         image = image.copy()\n",
    "\n",
    "\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6e556e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"../dataset/crop_224/\"\n",
    "labels_dir = \"../dataset/AppleV_intrinsic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edf61578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppleDataset(Dataset): \n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        self.labels = pd.read_csv(labels_dir)\n",
    "        self.images = os.listdir(images_dir)\n",
    "\n",
    "        # load the images and labels in a list\n",
    "        self.img_data = []\n",
    "        self.img_labels = []\n",
    "\n",
    "        for image in self.images:\n",
    "            img_path = os.path.join(images_dir, image)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = np.transpose(img, [2, 0, 1])\n",
    "            img = np.float32(img) / 255.0   # convert image to float32 [0,1]\n",
    "            \n",
    "            if img.shape != (3, 224, 224):\n",
    "                print(\"Skipping:\", image)\n",
    "                continue\n",
    "            self.img_data.append(img)\n",
    "\n",
    "            # match labels\n",
    "            image = image.split(\"_\")[0] + '.jpg'\n",
    "            t = self.labels[self.labels['filenames'] == image][\n",
    "                ['TSS_brix', 'Firmness', 'Titerabile acid percentage']\n",
    "            ].values\n",
    "\n",
    "            # Convert label to float32 and flatten\n",
    "            t = t.astype(np.float32).flatten()\n",
    "            self.img_labels.append(t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)   # safer than self.images\n",
    "\n",
    "    def arguement(self, img, rotTimes, vFlip, hFlip):\n",
    "        # Random rotation\n",
    "        for j in range(rotTimes):\n",
    "            img = np.rot90(img.copy(), axes=(1, 2))\n",
    "        # Random vertical Flip\n",
    "        for j in range(vFlip):\n",
    "            img = img[:, :, ::-1].copy()\n",
    "        # Random horizontal Flip\n",
    "        for j in range(hFlip):\n",
    "            img = img[:, ::-1, :].copy()\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img_data[idx]\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        rotTimes = random.randint(0, 3)\n",
    "        vFlip = random.randint(0, 1)\n",
    "        hFlip = random.randint(0, 1)\n",
    "        image = self.arguement(image, rotTimes, vFlip, hFlip)\n",
    "        \n",
    "        return image.copy(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e43acd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AppleDataset(images_dir, labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae11707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, input_size=(3, 224, 224), output_size=3, finetune=False):\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        # Load pre-trained VGG19 model\n",
    "        model = models.vgg19(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze convolutional layers\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = not finetune\n",
    "\n",
    "        # Modify the classifier: to reduce the number of params \n",
    "        model.classifier[0] = nn.Linear(25088, 2048)  # Output layer with the desired output size\n",
    "        model.classifier[3] = nn.Linear(2048, 2048)\n",
    "        model.classifier[6] = nn.Linear(2048, output_size)\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164af8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, input_size=(3, 224, 224), output_size=3, finetune=False, variant=\"b0\"):\n",
    "        super(EfficientNet, self).__init__()\n",
    "\n",
    "        # Load pretrained EfficientNet\n",
    "        if variant == \"b0\":\n",
    "            model = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "        elif variant == \"b1\":\n",
    "            model = models.efficientnet_b1(weights=\"IMAGENET1K_V1\")\n",
    "        elif variant == \"b2\":\n",
    "            model = models.efficientnet_b2(weights=\"IMAGENET1K_V1\")\n",
    "        else:\n",
    "            raise ValueError(f\"EfficientNet variant {variant} not supported here.\")\n",
    "\n",
    "        # Freeze backbone if not finetuning\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = finetune\n",
    "\n",
    "        # Replace classifier (EfficientNet has classifier[1] as final layer)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77461ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_size=(3, 224, 224), output_size=3, finetune=False, variant=\"b_16\"):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Load pretrained Vision Transformer\n",
    "        if variant == \"b_16\":\n",
    "            model = models.vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        elif variant == \"b_32\":\n",
    "            model = models.vit_b_32(weights=\"IMAGENET1K_V1\")\n",
    "        else:\n",
    "            raise ValueError(f\"ViT variant {variant} not supported here.\")\n",
    "\n",
    "        # Freeze backbone if not finetuning\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = finetune\n",
    "\n",
    "        # Replace the head (classifier)\n",
    "        in_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c06c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "\n",
    "class SwinT(nn.Module):\n",
    "    def __init__(self, input_size=(3, 224, 224), output_size=3, finetune=False):\n",
    "        super(SwinT, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Swin Transformer Tiny model\n",
    "        model = swin_t(weights=Swin_T_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Freeze layers if finetune=False\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = finetune  # True if finetuning\n",
    "\n",
    "        # Replace classifier head for regression\n",
    "        in_features = model.head.in_features\n",
    "        model.head = nn.Linear(in_features, output_size)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c54464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "def write_to_csv(model_name, num_epochs, train_losses, val_losses, train_metrics, val_metrics, total_params, trainable_params, non_trainable_params):\n",
    "    # Save results to CSV\n",
    "    results = {\n",
    "        \"Epoch\": list(range(1, num_epochs + 1)),\n",
    "        \"Train Loss\": train_losses,\n",
    "        \"Train MSE\": train_metrics[\"MSE\"],\n",
    "        \"Train MAE\": train_metrics[\"MAE\"],\n",
    "        \"Train R2\": train_metrics[\"R2\"],\n",
    "    }\n",
    "    \n",
    "    # Add parameter counts as additional rows (repeated across all rows for clarity)\n",
    "    results[\"Total Params\"] = [total_params] * num_epochs\n",
    "    results[\"Trainable Params\"] = [trainable_params] * num_epochs\n",
    "    results[\"Non-trainable Params\"] = [non_trainable_params] * num_epochs\n",
    "\n",
    "    results[\"Val Loss\"] = val_losses * num_epochs\n",
    "    results[\"Val MSE\"] = val_metrics[\"MSE\"] * num_epochs\n",
    "    results[\"Val MAE\"] = val_metrics[\"MAE\"] * num_epochs\n",
    "    results[\"Val R2\"] = val_metrics[\"R2\"] * num_epochs\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f\"results/{model_name}.csv\", index=False)\n",
    "    print(\"Results saved to 'training_results_with_params.csv'\")\n",
    "\n",
    "def calculate_model_parameters(model):\n",
    "    # Count model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "    return total_params, trainable_params, non_trainable_params\n",
    "\n",
    "\n",
    "def train_model(model_name, model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    total_params, trainable_params, non_trainable_params = calculate_model_parameters(model)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_train_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_metrics = {\"MSE\": [], \"MAE\": [], \"R2\": []}\n",
    "    val_metrics = {\"MSE\": [], \"MAE\": [], \"R2\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss and predictions\n",
    "            running_loss += loss.item()\n",
    "            y_true_train.append(labels)\n",
    "            y_pred_train.append(outputs)\n",
    "            break\n",
    "\n",
    "        # Normalize training loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        y_true_train = torch.cat(y_true_train, dim=0)\n",
    "        y_pred_train = torch.cat(y_pred_train, dim=0)\n",
    "        mse, mae, r2 = calculate_metrics(y_true_train, y_pred_train)\n",
    "        train_metrics[\"MSE\"].append(mse)\n",
    "        train_metrics[\"MAE\"].append(mae)\n",
    "        train_metrics[\"R2\"].append(r2)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Optionally save the best model\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_true_val, y_pred_val = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss and predictions\n",
    "            val_loss += loss.item()\n",
    "            y_true_val.append(labels)\n",
    "            y_pred_val.append(outputs)\n",
    "            break\n",
    "\n",
    "    # Normalize validation loss\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    y_true_val = torch.cat(y_true_val, dim=0)\n",
    "    y_pred_val = torch.cat(y_pred_val, dim=0)\n",
    "    mse, mae, r2 = calculate_metrics(y_true_val, y_pred_val)\n",
    "    val_metrics[\"MSE\"].append(mse)\n",
    "    val_metrics[\"MAE\"].append(mae)\n",
    "    val_metrics[\"R2\"].append(r2)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "    ## Save the model's state dictionary\n",
    "    # torch.save(model.state_dict(), f\"saved_models/{model_name}.pth\")\n",
    "    # print(f'Model saved at saved_models/{model_name}.pth')\n",
    "\n",
    "    # write_to_csv(model_name, num_epochs, train_losses, val_losses, train_metrics, val_metrics, total_params, trainable_params, non_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "934d6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf131b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = AppleDataset(images_dir, labels_dir)\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b236d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adfcdf8cdf74c3bac7b6208b66458d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 86193923\n",
      "Trainable parameters: 395267\n",
      "Non-trainable parameters: 85798656\n",
      "Epoch [1/100], Training Loss: 0.0002, MSE: 0.2434, MAE: 0.4274, R2: -7.3954\n",
      "Epoch [2/100], Training Loss: 0.0001, MSE: 0.1753, MAE: 0.3248, R2: -3.4889\n",
      "Epoch [3/100], Training Loss: 0.0002, MSE: 0.2331, MAE: 0.4100, R2: -7.9920\n",
      "Epoch [4/100], Training Loss: 0.0001, MSE: 0.0742, MAE: 0.2002, R2: -2.7736\n",
      "Epoch [5/100], Training Loss: 0.0001, MSE: 0.0877, MAE: 0.2389, R2: -2.0282\n",
      "Epoch [6/100], Training Loss: 0.0001, MSE: 0.0978, MAE: 0.2468, R2: -2.8920\n",
      "Epoch [7/100], Training Loss: 0.0001, MSE: 0.1366, MAE: 0.2986, R2: -6.7691\n",
      "Epoch [8/100], Training Loss: 0.0001, MSE: 0.1258, MAE: 0.2929, R2: -2.6513\n",
      "Epoch [9/100], Training Loss: 0.0001, MSE: 0.0665, MAE: 0.2199, R2: -1.5265\n",
      "Epoch [10/100], Training Loss: 0.0000, MSE: 0.0618, MAE: 0.2095, R2: -0.9019\n",
      "Epoch [11/100], Training Loss: 0.0000, MSE: 0.0559, MAE: 0.1909, R2: -0.6507\n",
      "Epoch [12/100], Training Loss: 0.0000, MSE: 0.0525, MAE: 0.1966, R2: -0.9544\n",
      "Epoch [13/100], Training Loss: 0.0000, MSE: 0.0557, MAE: 0.1789, R2: -0.9353\n",
      "Epoch [14/100], Training Loss: 0.0001, MSE: 0.0844, MAE: 0.2281, R2: -1.9059\n",
      "Epoch [15/100], Training Loss: 0.0001, MSE: 0.1013, MAE: 0.2620, R2: -2.5697\n",
      "Epoch [16/100], Training Loss: 0.0000, MSE: 0.0579, MAE: 0.2036, R2: -2.2259\n",
      "Epoch [17/100], Training Loss: 0.0000, MSE: 0.0332, MAE: 0.1410, R2: -1.1421\n",
      "Epoch [18/100], Training Loss: 0.0000, MSE: 0.0411, MAE: 0.1598, R2: -1.0126\n",
      "Epoch [19/100], Training Loss: 0.0001, MSE: 0.0714, MAE: 0.2084, R2: -1.1562\n",
      "Epoch [20/100], Training Loss: 0.0000, MSE: 0.0600, MAE: 0.2077, R2: -0.7150\n",
      "Epoch [21/100], Training Loss: 0.0000, MSE: 0.0422, MAE: 0.1636, R2: -0.9847\n",
      "Epoch [22/100], Training Loss: 0.0000, MSE: 0.0575, MAE: 0.1812, R2: -1.5796\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [23/100], Training Loss: 0.0000, MSE: 0.0485, MAE: 0.1826, R2: -0.9441\n",
      "Epoch [24/100], Training Loss: 0.0001, MSE: 0.0729, MAE: 0.2131, R2: -0.9263\n",
      "Epoch [25/100], Training Loss: 0.0000, MSE: 0.0441, MAE: 0.1488, R2: -0.4686\n",
      "Epoch [26/100], Training Loss: 0.0000, MSE: 0.0354, MAE: 0.1435, R2: -0.8561\n",
      "Epoch [27/100], Training Loss: 0.0000, MSE: 0.0429, MAE: 0.1671, R2: -0.9447\n",
      "Epoch [28/100], Training Loss: 0.0000, MSE: 0.0271, MAE: 0.1341, R2: -0.2767\n",
      "Epoch [29/100], Training Loss: 0.0000, MSE: 0.0488, MAE: 0.1858, R2: -0.9490\n",
      "Epoch [30/100], Training Loss: 0.0000, MSE: 0.0576, MAE: 0.1977, R2: -0.6420\n",
      "Epoch [31/100], Training Loss: 0.0000, MSE: 0.0487, MAE: 0.1793, R2: -0.4201\n",
      "Epoch [32/100], Training Loss: 0.0000, MSE: 0.0269, MAE: 0.1282, R2: -1.5530\n",
      "Epoch [33/100], Training Loss: 0.0000, MSE: 0.0550, MAE: 0.1875, R2: -1.2070\n",
      "Epoch [34/100], Training Loss: 0.0000, MSE: 0.0583, MAE: 0.1877, R2: -1.1855\n",
      "Epoch [35/100], Training Loss: 0.0001, MSE: 0.0764, MAE: 0.2309, R2: -0.9243\n",
      "Epoch [36/100], Training Loss: 0.0000, MSE: 0.0271, MAE: 0.1305, R2: -0.1707\n",
      "Epoch [37/100], Training Loss: 0.0000, MSE: 0.0537, MAE: 0.1823, R2: -0.3102\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [38/100], Training Loss: 0.0000, MSE: 0.0594, MAE: 0.1975, R2: -0.3400\n",
      "Epoch [39/100], Training Loss: 0.0000, MSE: 0.0367, MAE: 0.1507, R2: -0.4140\n",
      "Epoch [40/100], Training Loss: 0.0000, MSE: 0.0430, MAE: 0.1557, R2: -0.2816\n",
      "Epoch [41/100], Training Loss: 0.0000, MSE: 0.0429, MAE: 0.1714, R2: -0.8941\n",
      "Epoch [42/100], Training Loss: 0.0000, MSE: 0.0564, MAE: 0.2005, R2: -1.6474\n",
      "Epoch [43/100], Training Loss: 0.0000, MSE: 0.0553, MAE: 0.1854, R2: -1.5045\n",
      "Epoch [44/100], Training Loss: 0.0000, MSE: 0.0446, MAE: 0.1709, R2: -0.3623\n",
      "Epoch [45/100], Training Loss: 0.0000, MSE: 0.0351, MAE: 0.1494, R2: -0.3217\n",
      "Epoch [46/100], Training Loss: 0.0000, MSE: 0.0439, MAE: 0.1650, R2: -0.5671\n",
      "Epoch [47/100], Training Loss: 0.0000, MSE: 0.0424, MAE: 0.1595, R2: -0.6365\n",
      "Epoch [48/100], Training Loss: 0.0000, MSE: 0.0558, MAE: 0.1908, R2: -0.5022\n",
      "Epoch [49/100], Training Loss: 0.0000, MSE: 0.0529, MAE: 0.1727, R2: -0.6935\n",
      "Epoch [50/100], Training Loss: 0.0000, MSE: 0.0460, MAE: 0.1773, R2: -0.5001\n",
      "Epoch [51/100], Training Loss: 0.0000, MSE: 0.0321, MAE: 0.1412, R2: -0.4630\n",
      "Epoch [52/100], Training Loss: 0.0000, MSE: 0.0443, MAE: 0.1722, R2: -0.6018\n",
      "Epoch [53/100], Training Loss: 0.0000, MSE: 0.0547, MAE: 0.1853, R2: -0.9436\n",
      "Epoch [54/100], Training Loss: 0.0000, MSE: 0.0463, MAE: 0.1666, R2: -0.1126\n",
      "Epoch [55/100], Training Loss: 0.0000, MSE: 0.0567, MAE: 0.1776, R2: -1.9392\n",
      "Epoch [56/100], Training Loss: 0.0000, MSE: 0.0420, MAE: 0.1628, R2: -0.9381\n",
      "Epoch [57/100], Training Loss: 0.0000, MSE: 0.0486, MAE: 0.1712, R2: -0.7592\n",
      "Epoch [58/100], Training Loss: 0.0000, MSE: 0.0412, MAE: 0.1555, R2: -0.3532\n",
      "Epoch [59/100], Training Loss: 0.0000, MSE: 0.0410, MAE: 0.1592, R2: -0.9866\n",
      "Epoch [60/100], Training Loss: 0.0000, MSE: 0.0448, MAE: 0.1668, R2: -0.8102\n",
      "Epoch [61/100], Training Loss: 0.0000, MSE: 0.0468, MAE: 0.1774, R2: -1.0605\n",
      "Epoch [62/100], Training Loss: 0.0000, MSE: 0.0415, MAE: 0.1528, R2: -1.5588\n",
      "Epoch [63/100], Training Loss: 0.0000, MSE: 0.0324, MAE: 0.1419, R2: -0.9911\n",
      "Epoch [64/100], Training Loss: 0.0000, MSE: 0.0551, MAE: 0.1817, R2: -0.4932\n",
      "Epoch [65/100], Training Loss: 0.0000, MSE: 0.0472, MAE: 0.1723, R2: -0.4099\n",
      "Epoch [66/100], Training Loss: 0.0000, MSE: 0.0286, MAE: 0.1431, R2: -0.5018\n",
      "Epoch [67/100], Training Loss: 0.0000, MSE: 0.0288, MAE: 0.1257, R2: -0.2023\n",
      "Epoch [68/100], Training Loss: 0.0000, MSE: 0.0385, MAE: 0.1493, R2: -0.2582\n",
      "Epoch [69/100], Training Loss: 0.0000, MSE: 0.0632, MAE: 0.1925, R2: -0.8186\n",
      "Epoch [70/100], Training Loss: 0.0000, MSE: 0.0265, MAE: 0.1354, R2: -0.7160\n",
      "Epoch [71/100], Training Loss: 0.0000, MSE: 0.0370, MAE: 0.1517, R2: -0.3838\n",
      "Epoch [72/100], Training Loss: 0.0000, MSE: 0.0272, MAE: 0.1346, R2: -0.0936\n",
      "Epoch [73/100], Training Loss: 0.0000, MSE: 0.0369, MAE: 0.1416, R2: -0.5760\n",
      "Epoch [74/100], Training Loss: 0.0000, MSE: 0.0288, MAE: 0.1340, R2: 0.0331\n",
      "Epoch [75/100], Training Loss: 0.0000, MSE: 0.0318, MAE: 0.1513, R2: -0.3809\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [76/100], Training Loss: 0.0000, MSE: 0.0445, MAE: 0.1570, R2: -1.0451\n",
      "Epoch [77/100], Training Loss: 0.0000, MSE: 0.0398, MAE: 0.1676, R2: -0.3046\n",
      "Epoch [78/100], Training Loss: 0.0000, MSE: 0.0308, MAE: 0.1380, R2: -0.5252\n",
      "Epoch [79/100], Training Loss: 0.0000, MSE: 0.0534, MAE: 0.1748, R2: -0.3141\n",
      "Epoch [80/100], Training Loss: 0.0000, MSE: 0.0441, MAE: 0.1659, R2: -0.5807\n",
      "Epoch [81/100], Training Loss: 0.0000, MSE: 0.0618, MAE: 0.1939, R2: -0.6760\n",
      "Epoch [82/100], Training Loss: 0.0000, MSE: 0.0307, MAE: 0.1356, R2: -0.5549\n",
      "Epoch [83/100], Training Loss: 0.0000, MSE: 0.0475, MAE: 0.1755, R2: -0.3389\n",
      "Epoch [84/100], Training Loss: 0.0000, MSE: 0.0434, MAE: 0.1620, R2: -0.5395\n",
      "Epoch [85/100], Training Loss: 0.0000, MSE: 0.0378, MAE: 0.1305, R2: -0.6312\n",
      "Epoch [86/100], Training Loss: 0.0000, MSE: 0.0315, MAE: 0.1398, R2: -0.2129\n",
      "Epoch [87/100], Training Loss: 0.0000, MSE: 0.0425, MAE: 0.1543, R2: 0.0118\n",
      "Epoch [88/100], Training Loss: 0.0000, MSE: 0.0448, MAE: 0.1633, R2: -0.1278\n",
      "Epoch [89/100], Training Loss: 0.0000, MSE: 0.0459, MAE: 0.1735, R2: -0.6348\n",
      "Epoch [90/100], Training Loss: 0.0000, MSE: 0.0441, MAE: 0.1636, R2: -0.5067\n",
      "Epoch [91/100], Training Loss: 0.0000, MSE: 0.0541, MAE: 0.1787, R2: -0.4628\n",
      "Epoch [92/100], Training Loss: 0.0000, MSE: 0.0347, MAE: 0.1477, R2: -0.8554\n",
      "Epoch [93/100], Training Loss: 0.0000, MSE: 0.0435, MAE: 0.1661, R2: -0.4726\n",
      "Epoch [94/100], Training Loss: 0.0000, MSE: 0.0399, MAE: 0.1635, R2: -0.6366\n",
      "Epoch [95/100], Training Loss: 0.0000, MSE: 0.0313, MAE: 0.1370, R2: -0.4396\n",
      "Epoch [96/100], Training Loss: 0.0000, MSE: 0.0410, MAE: 0.1642, R2: -0.2715\n",
      "Epoch [97/100], Training Loss: 0.0000, MSE: 0.0394, MAE: 0.1637, R2: -0.4947\n",
      "Epoch [98/100], Training Loss: 0.0000, MSE: 0.0380, MAE: 0.1562, R2: -0.3790\n",
      "Epoch [99/100], Training Loss: 0.0000, MSE: 0.0353, MAE: 0.1563, R2: -0.1668\n",
      "Epoch [100/100], Training Loss: 0.0000, MSE: 0.0521, MAE: 0.1750, R2: -0.7977\n",
      "Epoch [100/100], Validation Loss: 0.0001, MSE: 0.0274, MAE: 0.1313, R2: -0.6258\n"
     ]
    }
   ],
   "source": [
    "# Create ViT model object\n",
    "model = ViT(input_size=(3, 224, 224), output_size=3, finetune=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # since you're doing regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Device\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Train\n",
    "num_epochs = 100\n",
    "train_model(model, model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f5a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 75609155\n",
      "Trainable parameters: 75609155\n",
      "Non-trainable parameters: 0\n",
      "Epoch [1/100], Training Loss: 0.0003, MSE: 0.3351, MAE: 0.5283, R2: -8.9776\n",
      "Epoch [2/100], Training Loss: 0.4189, MSE: 537.3878, MAE: 22.1737, R2: -37414.7030\n",
      "Epoch [3/100], Training Loss: 0.0001, MSE: 0.0730, MAE: 0.2142, R2: -1.2208\n",
      "Epoch [4/100], Training Loss: 0.0005, MSE: 0.6754, MAE: 0.6250, R2: -17.3083\n",
      "Epoch [5/100], Training Loss: 0.0003, MSE: 0.4095, MAE: 0.5763, R2: -15.2062\n",
      "Epoch [6/100], Training Loss: 0.0001, MSE: 0.1345, MAE: 0.3063, R2: -2.9594\n",
      "Epoch [7/100], Training Loss: 0.0379, MSE: 48.6379, MAE: 6.2733, R2: -1937.3174\n",
      "Epoch [8/100], Training Loss: 0.0099, MSE: 12.7643, MAE: 3.1725, R2: -376.1092\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [9/100], Training Loss: 0.0002, MSE: 0.2720, MAE: 0.4903, R2: -12.8923\n",
      "Epoch [10/100], Training Loss: 0.0001, MSE: 0.0653, MAE: 0.2010, R2: -0.8495\n",
      "Epoch [11/100], Training Loss: 0.0001, MSE: 0.1126, MAE: 0.2744, R2: -3.6720\n",
      "Epoch [12/100], Training Loss: 0.0000, MSE: 0.0597, MAE: 0.1884, R2: -1.8670\n",
      "Epoch [13/100], Training Loss: 0.0003, MSE: 0.3697, MAE: 0.5543, R2: -13.5786\n",
      "Epoch [14/100], Training Loss: 0.0002, MSE: 0.2336, MAE: 0.4496, R2: -7.4724\n",
      "Epoch [15/100], Training Loss: 0.0001, MSE: 0.1024, MAE: 0.2812, R2: -3.9804\n",
      "Epoch [16/100], Training Loss: 0.0024, MSE: 3.1029, MAE: 1.5359, R2: -144.5105\n",
      "Epoch [17/100], Training Loss: 0.0002, MSE: 0.2025, MAE: 0.4039, R2: -5.1399\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [18/100], Training Loss: 0.0002, MSE: 0.2673, MAE: 0.4866, R2: -12.9862\n",
      "Epoch [19/100], Training Loss: 0.0003, MSE: 0.3301, MAE: 0.5338, R2: -10.1554\n",
      "Epoch [20/100], Training Loss: 0.0002, MSE: 0.2552, MAE: 0.4713, R2: -13.9545\n",
      "Epoch [21/100], Training Loss: 0.0002, MSE: 0.3062, MAE: 0.5043, R2: -15.3779\n",
      "Epoch [22/100], Training Loss: 0.0002, MSE: 0.3059, MAE: 0.5134, R2: -15.2409\n",
      "Epoch [23/100], Training Loss: 0.0002, MSE: 0.2590, MAE: 0.4760, R2: -10.8588\n",
      "Epoch [24/100], Training Loss: 0.0002, MSE: 0.3119, MAE: 0.5172, R2: -18.5871\n",
      "Epoch [25/100], Training Loss: 0.0002, MSE: 0.2958, MAE: 0.4993, R2: -9.8266\n",
      "Epoch [26/100], Training Loss: 0.0002, MSE: 0.2993, MAE: 0.5154, R2: -14.7572\n",
      "Epoch [27/100], Training Loss: 0.0002, MSE: 0.3119, MAE: 0.5199, R2: -16.4204\n",
      "Epoch [28/100], Training Loss: 0.0002, MSE: 0.3148, MAE: 0.5260, R2: -10.9548\n",
      "Epoch [29/100], Training Loss: 0.0002, MSE: 0.3146, MAE: 0.5164, R2: -20.6306\n",
      "Epoch [30/100], Training Loss: 0.0002, MSE: 0.3058, MAE: 0.5152, R2: -14.9738\n",
      "Epoch [31/100], Training Loss: 0.0002, MSE: 0.2382, MAE: 0.4441, R2: -11.3310\n",
      "Epoch [32/100], Training Loss: 0.0002, MSE: 0.3188, MAE: 0.5208, R2: -9.8190\n",
      "Epoch [33/100], Training Loss: 0.0003, MSE: 0.3210, MAE: 0.5305, R2: -22.1638\n",
      "Epoch [34/100], Training Loss: 0.0002, MSE: 0.2605, MAE: 0.4558, R2: -7.6422\n",
      "Epoch [35/100], Training Loss: 0.0002, MSE: 0.2714, MAE: 0.4760, R2: -9.4432\n",
      "Epoch [36/100], Training Loss: 0.0002, MSE: 0.2575, MAE: 0.4519, R2: -4.8140\n",
      "Epoch [37/100], Training Loss: 0.0002, MSE: 0.3014, MAE: 0.5042, R2: -8.1476\n",
      "Epoch [38/100], Training Loss: 0.0002, MSE: 0.2491, MAE: 0.4577, R2: -9.8722\n",
      "Epoch [39/100], Training Loss: 0.0002, MSE: 0.2409, MAE: 0.4379, R2: -5.9872\n",
      "Epoch [40/100], Training Loss: 0.0002, MSE: 0.2132, MAE: 0.4147, R2: -7.1511\n",
      "Epoch [41/100], Training Loss: 0.0002, MSE: 0.1965, MAE: 0.3911, R2: -5.3010\n",
      "Epoch [42/100], Training Loss: 0.0002, MSE: 0.2214, MAE: 0.4151, R2: -10.4886\n",
      "Epoch [43/100], Training Loss: 0.0002, MSE: 0.2108, MAE: 0.4054, R2: -4.7177\n",
      "Epoch [44/100], Training Loss: 0.0001, MSE: 0.1907, MAE: 0.3932, R2: -5.7436\n",
      "Epoch [45/100], Training Loss: 0.0001, MSE: 0.1588, MAE: 0.3416, R2: -6.1561\n",
      "Epoch [46/100], Training Loss: 0.0001, MSE: 0.1423, MAE: 0.3258, R2: -7.6557\n",
      "Epoch [47/100], Training Loss: 0.0001, MSE: 0.1595, MAE: 0.3237, R2: -1.8801\n",
      "Epoch [48/100], Training Loss: 0.0001, MSE: 0.1378, MAE: 0.3046, R2: -2.0770\n",
      "Epoch [49/100], Training Loss: 0.0001, MSE: 0.0955, MAE: 0.2513, R2: -4.1680\n",
      "Epoch [50/100], Training Loss: 0.0001, MSE: 0.0777, MAE: 0.2272, R2: -0.9848\n",
      "Epoch [51/100], Training Loss: 0.0001, MSE: 0.0926, MAE: 0.2403, R2: -2.5375\n",
      "Epoch [52/100], Training Loss: 0.0001, MSE: 0.0655, MAE: 0.2052, R2: -1.0438\n",
      "Epoch [53/100], Training Loss: 0.0000, MSE: 0.0468, MAE: 0.1719, R2: -0.2682\n",
      "Epoch [54/100], Training Loss: 0.0000, MSE: 0.0447, MAE: 0.1657, R2: -0.4879\n",
      "Epoch [55/100], Training Loss: 0.0000, MSE: 0.0441, MAE: 0.1654, R2: -0.7504\n",
      "Epoch [56/100], Training Loss: 0.0000, MSE: 0.0375, MAE: 0.1545, R2: -0.7683\n",
      "Epoch [57/100], Training Loss: 0.0000, MSE: 0.0506, MAE: 0.1843, R2: -0.1485\n",
      "Epoch [58/100], Training Loss: 0.0000, MSE: 0.0615, MAE: 0.1959, R2: -1.4818\n",
      "Epoch [59/100], Training Loss: 0.0001, MSE: 0.0703, MAE: 0.2017, R2: -1.5979\n",
      "Epoch [60/100], Training Loss: 0.0001, MSE: 0.0886, MAE: 0.2387, R2: -1.5456\n",
      "Epoch [61/100], Training Loss: 0.0000, MSE: 0.0563, MAE: 0.1957, R2: -0.6892\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [62/100], Training Loss: 0.0000, MSE: 0.0294, MAE: 0.1428, R2: -0.3409\n",
      "Epoch [63/100], Training Loss: 0.0000, MSE: 0.0410, MAE: 0.1558, R2: -0.8422\n",
      "Epoch [64/100], Training Loss: 0.0000, MSE: 0.0456, MAE: 0.1725, R2: -1.5606\n",
      "Epoch [65/100], Training Loss: 0.0000, MSE: 0.0611, MAE: 0.1798, R2: -0.3621\n",
      "Epoch [66/100], Training Loss: 0.0000, MSE: 0.0349, MAE: 0.1465, R2: -0.4435\n",
      "Epoch [67/100], Training Loss: 0.0000, MSE: 0.0506, MAE: 0.1639, R2: -0.3229\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [68/100], Training Loss: 0.0000, MSE: 0.0552, MAE: 0.1721, R2: -0.4027\n",
      "Epoch [69/100], Training Loss: 0.0000, MSE: 0.0369, MAE: 0.1542, R2: -0.3307\n",
      "Epoch [70/100], Training Loss: 0.0000, MSE: 0.0480, MAE: 0.1686, R2: -0.9148\n",
      "Epoch [71/100], Training Loss: 0.0000, MSE: 0.0423, MAE: 0.1649, R2: -0.7999\n",
      "Epoch [72/100], Training Loss: 0.0000, MSE: 0.0220, MAE: 0.1216, R2: -0.1456\n",
      "Epoch [73/100], Training Loss: 0.0000, MSE: 0.0501, MAE: 0.1788, R2: -0.7005\n",
      "Epoch [74/100], Training Loss: 0.0000, MSE: 0.0467, MAE: 0.1634, R2: -0.6287\n",
      "Epoch [75/100], Training Loss: 0.0000, MSE: 0.0343, MAE: 0.1488, R2: -0.5289\n",
      "Epoch [76/100], Training Loss: 0.0000, MSE: 0.0455, MAE: 0.1765, R2: -0.1751\n",
      "Epoch [77/100], Training Loss: 0.0000, MSE: 0.0372, MAE: 0.1547, R2: -1.1107\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [78/100], Training Loss: 0.0001, MSE: 0.0656, MAE: 0.2109, R2: -0.5120\n",
      "Epoch [79/100], Training Loss: 0.0000, MSE: 0.0310, MAE: 0.1451, R2: -0.0967\n",
      "Epoch [80/100], Training Loss: 0.0000, MSE: 0.0385, MAE: 0.1562, R2: -0.2978\n",
      "Epoch [81/100], Training Loss: 0.0000, MSE: 0.0532, MAE: 0.1821, R2: -0.2962\n",
      "Epoch [82/100], Training Loss: 0.0000, MSE: 0.0504, MAE: 0.1713, R2: -0.3988\n",
      "Epoch [83/100], Training Loss: 0.0000, MSE: 0.0417, MAE: 0.1657, R2: -0.3194\n",
      "Epoch [84/100], Training Loss: 0.0000, MSE: 0.0453, MAE: 0.1527, R2: -0.5617\n",
      "Epoch [85/100], Training Loss: 0.0000, MSE: 0.0532, MAE: 0.1790, R2: -0.5253\n",
      "Epoch [86/100], Training Loss: 0.0000, MSE: 0.0411, MAE: 0.1661, R2: -0.5777\n",
      "Epoch [87/100], Training Loss: 0.0000, MSE: 0.0389, MAE: 0.1542, R2: -0.7011\n",
      "Epoch [88/100], Training Loss: 0.0000, MSE: 0.0465, MAE: 0.1679, R2: -0.1757\n",
      "Epoch [89/100], Training Loss: 0.0000, MSE: 0.0264, MAE: 0.1383, R2: -0.1818\n",
      "Epoch [90/100], Training Loss: 0.0000, MSE: 0.0408, MAE: 0.1597, R2: -0.3424\n",
      "Epoch [91/100], Training Loss: 0.0000, MSE: 0.0508, MAE: 0.1710, R2: -0.1820\n",
      "Epoch [92/100], Training Loss: 0.0000, MSE: 0.0255, MAE: 0.1259, R2: -0.0379\n",
      "Epoch [93/100], Training Loss: 0.0000, MSE: 0.0301, MAE: 0.1428, R2: -1.1077\n",
      "Epoch [94/100], Training Loss: 0.0000, MSE: 0.0471, MAE: 0.1615, R2: -0.1711\n",
      "Epoch [95/100], Training Loss: 0.0000, MSE: 0.0442, MAE: 0.1699, R2: -0.2637\n",
      "Epoch [96/100], Training Loss: 0.0000, MSE: 0.0544, MAE: 0.1841, R2: -0.3299\n",
      "Epoch [97/100], Training Loss: 0.0000, MSE: 0.0419, MAE: 0.1574, R2: -0.5776\n",
      "Epoch [98/100], Training Loss: 0.0000, MSE: 0.0391, MAE: 0.1436, R2: -0.2751\n",
      "Epoch [99/100], Training Loss: 0.0000, MSE: 0.0330, MAE: 0.1411, R2: -0.3467\n",
      "Epoch [100/100], Training Loss: 0.0000, MSE: 0.0462, MAE: 0.1675, R2: -0.3369\n",
      "Epoch [100/100], Validation Loss: 0.0001, MSE: 0.0359, MAE: 0.1563, R2: -0.0341\n"
     ]
    }
   ],
   "source": [
    "model = VGG19(input_size=(3, 224, 224), output_size=3, finetune=False)\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # For regression (use BCEWithLogitsLoss for multi-label classification)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you have a Da taLoader ready as `data_loader`\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 100\n",
    "train_model(model, model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad7cec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 27521661\n",
      "Trainable parameters: 2307\n",
      "Non-trainable parameters: 27519354\n",
      "Epoch [1/100], Training Loss: 0.0003, MSE: 0.3593, MAE: 0.4992, R2: -6.5769\n",
      "Epoch [2/100], Training Loss: 0.0002, MSE: 0.2428, MAE: 0.3940, R2: -6.8238\n",
      "Epoch [3/100], Training Loss: 0.0002, MSE: 0.1961, MAE: 0.3778, R2: -5.7998\n",
      "Epoch [4/100], Training Loss: 0.0001, MSE: 0.1440, MAE: 0.2930, R2: -3.1157\n",
      "Epoch [5/100], Training Loss: 0.0001, MSE: 0.0657, MAE: 0.2092, R2: -2.6806\n",
      "Epoch [6/100], Training Loss: 0.0001, MSE: 0.1007, MAE: 0.2352, R2: -3.1264\n",
      "Epoch [7/100], Training Loss: 0.0000, MSE: 0.0556, MAE: 0.1957, R2: -1.7371\n",
      "Epoch [8/100], Training Loss: 0.0001, MSE: 0.0855, MAE: 0.2210, R2: -4.3238\n",
      "Epoch [9/100], Training Loss: 0.0000, MSE: 0.0617, MAE: 0.2126, R2: -1.5577\n",
      "Epoch [10/100], Training Loss: 0.0001, MSE: 0.1508, MAE: 0.2802, R2: -2.5503\n",
      "Epoch [11/100], Training Loss: 0.0001, MSE: 0.1208, MAE: 0.2630, R2: -5.1414\n",
      "Epoch [12/100], Training Loss: 0.0001, MSE: 0.1344, MAE: 0.2942, R2: -6.8882\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [13/100], Training Loss: 0.0001, MSE: 0.1269, MAE: 0.2464, R2: -3.5662\n",
      "Epoch [14/100], Training Loss: 0.0000, MSE: 0.0609, MAE: 0.1995, R2: -3.5270\n",
      "Epoch [15/100], Training Loss: 0.0001, MSE: 0.0737, MAE: 0.2155, R2: -1.9212\n",
      "Epoch [16/100], Training Loss: 0.0001, MSE: 0.0900, MAE: 0.2356, R2: -1.8928\n",
      "Epoch [17/100], Training Loss: 0.0000, MSE: 0.0489, MAE: 0.1765, R2: -0.7668\n",
      "Epoch [18/100], Training Loss: 0.0001, MSE: 0.0928, MAE: 0.2515, R2: -2.3652\n",
      "Epoch [19/100], Training Loss: 0.0000, MSE: 0.0515, MAE: 0.1910, R2: -0.7353\n",
      "Epoch [20/100], Training Loss: 0.0001, MSE: 0.0687, MAE: 0.2056, R2: -5.1075\n",
      "Epoch [21/100], Training Loss: 0.0001, MSE: 0.0946, MAE: 0.2420, R2: -2.4580\n",
      "Epoch [22/100], Training Loss: 0.0001, MSE: 0.0877, MAE: 0.2367, R2: -5.5696\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [23/100], Training Loss: 0.0001, MSE: 0.0804, MAE: 0.2181, R2: -3.4035\n",
      "Epoch [24/100], Training Loss: 0.0001, MSE: 0.0814, MAE: 0.2250, R2: -3.0904\n",
      "Epoch [25/100], Training Loss: 0.0001, MSE: 0.0803, MAE: 0.2153, R2: -2.5518\n",
      "Epoch [26/100], Training Loss: 0.0001, MSE: 0.0680, MAE: 0.1987, R2: -6.0012\n",
      "Epoch [27/100], Training Loss: 0.0001, MSE: 0.0856, MAE: 0.2278, R2: -2.3241\n",
      "Epoch [28/100], Training Loss: 0.0001, MSE: 0.0690, MAE: 0.2107, R2: -2.1128\n",
      "Epoch [29/100], Training Loss: 0.0001, MSE: 0.0869, MAE: 0.2313, R2: -1.3680\n",
      "Epoch [30/100], Training Loss: 0.0000, MSE: 0.0578, MAE: 0.1916, R2: -2.4094\n",
      "Epoch [31/100], Training Loss: 0.0000, MSE: 0.0618, MAE: 0.2055, R2: -0.5609\n",
      "Epoch [32/100], Training Loss: 0.0001, MSE: 0.0789, MAE: 0.2212, R2: -1.4855\n",
      "Epoch [33/100], Training Loss: 0.0001, MSE: 0.0668, MAE: 0.2036, R2: -0.9072\n",
      "Epoch [34/100], Training Loss: 0.0001, MSE: 0.0672, MAE: 0.1879, R2: -1.4772\n",
      "Epoch [35/100], Training Loss: 0.0001, MSE: 0.0801, MAE: 0.2195, R2: -2.1381\n",
      "Epoch [36/100], Training Loss: 0.0000, MSE: 0.0612, MAE: 0.2022, R2: -1.4778\n",
      "Epoch [37/100], Training Loss: 0.0001, MSE: 0.0777, MAE: 0.2306, R2: -1.8322\n",
      "Epoch [38/100], Training Loss: 0.0001, MSE: 0.0684, MAE: 0.2019, R2: -1.2711\n",
      "Epoch [39/100], Training Loss: 0.0000, MSE: 0.0549, MAE: 0.1963, R2: -0.8409\n",
      "Epoch [40/100], Training Loss: 0.0000, MSE: 0.0570, MAE: 0.1875, R2: -0.8521\n",
      "Epoch [41/100], Training Loss: 0.0001, MSE: 0.0687, MAE: 0.2072, R2: -0.5348\n",
      "Epoch [42/100], Training Loss: 0.0001, MSE: 0.0736, MAE: 0.2135, R2: -2.7648\n",
      "Epoch [43/100], Training Loss: 0.0000, MSE: 0.0632, MAE: 0.1919, R2: -0.8219\n",
      "Epoch [44/100], Training Loss: 0.0001, MSE: 0.0810, MAE: 0.2330, R2: -1.2768\n",
      "Epoch [45/100], Training Loss: 0.0001, MSE: 0.0661, MAE: 0.1911, R2: -1.5593\n",
      "Epoch [46/100], Training Loss: 0.0000, MSE: 0.0499, MAE: 0.1761, R2: -1.4944\n",
      "Epoch [47/100], Training Loss: 0.0000, MSE: 0.0582, MAE: 0.1941, R2: -0.7179\n",
      "Epoch [48/100], Training Loss: 0.0000, MSE: 0.0453, MAE: 0.1648, R2: -1.0575\n",
      "Epoch [49/100], Training Loss: 0.0000, MSE: 0.0536, MAE: 0.1790, R2: -1.0513\n",
      "Epoch [50/100], Training Loss: 0.0000, MSE: 0.0641, MAE: 0.2001, R2: -1.2906\n",
      "Epoch [51/100], Training Loss: 0.0001, MSE: 0.0692, MAE: 0.1905, R2: -2.5241\n",
      "Epoch [52/100], Training Loss: 0.0001, MSE: 0.0662, MAE: 0.2109, R2: -1.2978\n",
      "Epoch [53/100], Training Loss: 0.0000, MSE: 0.0442, MAE: 0.1697, R2: -0.9479\n",
      "Epoch [54/100], Training Loss: 0.0001, MSE: 0.0688, MAE: 0.2169, R2: -2.3385\n",
      "Epoch [55/100], Training Loss: 0.0001, MSE: 0.0777, MAE: 0.2052, R2: -2.1418\n",
      "Epoch [56/100], Training Loss: 0.0000, MSE: 0.0371, MAE: 0.1491, R2: -0.2968\n",
      "Epoch [57/100], Training Loss: 0.0000, MSE: 0.0552, MAE: 0.1913, R2: -1.3070\n",
      "Epoch [58/100], Training Loss: 0.0001, MSE: 0.0826, MAE: 0.2314, R2: -0.6765\n",
      "Epoch [59/100], Training Loss: 0.0000, MSE: 0.0372, MAE: 0.1475, R2: -3.3332\n",
      "Epoch [60/100], Training Loss: 0.0000, MSE: 0.0538, MAE: 0.1831, R2: -0.7782\n",
      "Epoch [61/100], Training Loss: 0.0001, MSE: 0.0888, MAE: 0.2002, R2: -2.3954\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [62/100], Training Loss: 0.0000, MSE: 0.0421, MAE: 0.1602, R2: -0.9504\n",
      "Epoch [63/100], Training Loss: 0.0001, MSE: 0.0650, MAE: 0.1962, R2: -1.0368\n",
      "Epoch [64/100], Training Loss: 0.0001, MSE: 0.0662, MAE: 0.1985, R2: -0.9409\n",
      "Epoch [65/100], Training Loss: 0.0000, MSE: 0.0541, MAE: 0.1839, R2: -0.9108\n",
      "Epoch [66/100], Training Loss: 0.0001, MSE: 0.0651, MAE: 0.2008, R2: -2.1523\n",
      "Epoch [67/100], Training Loss: 0.0000, MSE: 0.0596, MAE: 0.1964, R2: -0.8123\n",
      "Epoch [68/100], Training Loss: 0.0000, MSE: 0.0533, MAE: 0.1795, R2: -0.8045\n",
      "Epoch [69/100], Training Loss: 0.0000, MSE: 0.0368, MAE: 0.1424, R2: -0.7472\n",
      "Epoch [70/100], Training Loss: 0.0000, MSE: 0.0630, MAE: 0.1928, R2: -0.9143\n",
      "Epoch [71/100], Training Loss: 0.0000, MSE: 0.0541, MAE: 0.1873, R2: -1.0407\n",
      "Epoch [72/100], Training Loss: 0.0000, MSE: 0.0390, MAE: 0.1509, R2: -0.4191\n",
      "Epoch [73/100], Training Loss: 0.0001, MSE: 0.0700, MAE: 0.2147, R2: -0.5607\n",
      "Epoch [74/100], Training Loss: 0.0000, MSE: 0.0613, MAE: 0.1866, R2: -0.7398\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [75/100], Training Loss: 0.0001, MSE: 0.0665, MAE: 0.1958, R2: -1.7229\n",
      "Epoch [76/100], Training Loss: 0.0000, MSE: 0.0491, MAE: 0.1751, R2: -0.4787\n",
      "Epoch [77/100], Training Loss: 0.0000, MSE: 0.0420, MAE: 0.1622, R2: -1.6256\n",
      "Epoch [78/100], Training Loss: 0.0000, MSE: 0.0357, MAE: 0.1483, R2: -0.1565\n",
      "Epoch [79/100], Training Loss: 0.0000, MSE: 0.0335, MAE: 0.1418, R2: -0.2986\n",
      "Epoch [80/100], Training Loss: 0.0001, MSE: 0.0644, MAE: 0.1991, R2: -0.5412\n",
      "Epoch [81/100], Training Loss: 0.0000, MSE: 0.0390, MAE: 0.1594, R2: -0.5292\n",
      "Epoch [82/100], Training Loss: 0.0000, MSE: 0.0574, MAE: 0.1974, R2: -0.6298\n",
      "Epoch [83/100], Training Loss: 0.0000, MSE: 0.0325, MAE: 0.1348, R2: -0.1723\n",
      "Epoch [84/100], Training Loss: 0.0000, MSE: 0.0368, MAE: 0.1507, R2: -2.1737\n",
      "Epoch [85/100], Training Loss: 0.0000, MSE: 0.0438, MAE: 0.1512, R2: -0.4541\n",
      "Epoch [86/100], Training Loss: 0.0001, MSE: 0.0743, MAE: 0.2065, R2: -1.0792\n",
      "Epoch [87/100], Training Loss: 0.0000, MSE: 0.0476, MAE: 0.1709, R2: -1.4634\n",
      "Epoch [88/100], Training Loss: 0.0001, MSE: 0.0675, MAE: 0.2231, R2: -0.6392\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [89/100], Training Loss: 0.0000, MSE: 0.0433, MAE: 0.1640, R2: -1.3035\n",
      "Epoch [90/100], Training Loss: 0.0000, MSE: 0.0405, MAE: 0.1522, R2: -0.7484\n",
      "Epoch [91/100], Training Loss: 0.0000, MSE: 0.0433, MAE: 0.1625, R2: -1.8987\n",
      "Epoch [92/100], Training Loss: 0.0000, MSE: 0.0609, MAE: 0.1914, R2: -2.6342\n",
      "Epoch [93/100], Training Loss: 0.0001, MSE: 0.0661, MAE: 0.1981, R2: -1.1729\n",
      "Epoch [94/100], Training Loss: 0.0000, MSE: 0.0585, MAE: 0.1890, R2: -0.9721\n",
      "Epoch [95/100], Training Loss: 0.0000, MSE: 0.0284, MAE: 0.1410, R2: -0.6250\n",
      "Epoch [96/100], Training Loss: 0.0000, MSE: 0.0341, MAE: 0.1451, R2: -0.6766\n",
      "Epoch [97/100], Training Loss: 0.0000, MSE: 0.0460, MAE: 0.1606, R2: -1.0206\n",
      "Epoch [98/100], Training Loss: 0.0000, MSE: 0.0343, MAE: 0.1500, R2: -0.3850\n",
      "Epoch [99/100], Training Loss: 0.0000, MSE: 0.0554, MAE: 0.1886, R2: -1.2860\n",
      "Epoch [100/100], Training Loss: 0.0000, MSE: 0.0435, MAE: 0.1694, R2: -1.1130\n",
      "Early stopping triggered. No improvement for 5 epochs.\n",
      "Epoch [100/100], Validation Loss: 0.0002, MSE: 0.0513, MAE: 0.1814, R2: -1.1452\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SwinT(input_size=(3, 224, 224), output_size=3, finetune=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Device\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "num_epochs = 100\n",
    "train_model(model, model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27eaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e77b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
